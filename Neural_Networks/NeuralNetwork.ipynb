{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Neural Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = mnist[\"data\"],mnist[\"target\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000) (10, 60000) (784, 10000) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_train,X_test,y_train,y_test = X[:60000],X[60000:],Y[:60000],Y[60000:]\n",
    "\n",
    "\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train,y_train = X_train[shuffle_index],y_train[shuffle_index]\n",
    "\n",
    "X_train = X_train.T\n",
    "\n",
    "Y_train = np.zeros((y_train.size,10))\n",
    "for i in range(y_train.size):\n",
    "    currEle = y_train[i]\n",
    "    Y_train[i,int(currEle)] = 1\n",
    "Y_train = Y_train.T\n",
    "\n",
    "\n",
    "X_test = X_test.T\n",
    "Y_test = y_test.T\n",
    "print(X_train.shape,Y_train.shape,X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return(1 / (1 + np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0] \n",
    "    n_h = 32\n",
    "    n_y = Y.shape[0] \n",
    "    \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "   \n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "   \n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    cost -- cross-entropy \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_y,m = Y.shape\n",
    "    cost = 0\n",
    "    for opclass in range(n_y):\n",
    "        cost -= (np.dot(np.log(A2[opclass,:]),Y[opclass,:].T) + np.dot(np.log(1-A2[opclass,:]),(1-Y[opclass,:]).T)) / m\n",
    "   \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2,A1.T) / m\n",
    "    db2 = np.sum(dZ2,axis = 1,keepdims = True) / m\n",
    "    dZ1 = np.dot(W2.T,dZ2) *(1 - A1**2)\n",
    "    dW1 = np.dot(dZ1,X.T) / m\n",
    "    db1 = np.sum(dZ1,axis = 1,keepdims = True) / m\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 0.03):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "   \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "   \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 2500, print_cost=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    parameters = initialize_parameters(n_x,n_h,n_y)\n",
    "   \n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        \n",
    "        # Forward propagation. \n",
    "        A2, cache = forward_propagation(X,parameters)\n",
    "        \n",
    "        # Cost function. \n",
    "        cost = compute_cost(A2,Y,parameters)\n",
    " \n",
    "        # Backpropagation. \n",
    "        grads = backward_propagation(parameters,cache,X,Y)\n",
    " \n",
    "        # Gradient descent parameter update. \n",
    "        parameters = update_parameters(parameters,grads)\n",
    "        \n",
    "       \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    print(\"Cost after iteration\",num_iteraions,\" is\",cost)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model \n",
    "    \"\"\"\n",
    "   \n",
    "    A2, cache = forward_propagation(X,parameters)\n",
    "    A2 = A2.T\n",
    "    predictions = np.argmax(A2,axis = 1)\n",
    "    \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 6.995137\n",
      "Cost after iteration 100: 3.037606\n",
      "Cost after iteration 200: 2.464415\n",
      "Cost after iteration 300: 2.030368\n",
      "Cost after iteration 400: 1.759583\n",
      "Cost after iteration 500: 1.527131\n",
      "Cost after iteration 600: 1.375309\n",
      "Cost after iteration 700: 1.260354\n",
      "Cost after iteration 800: 1.150775\n",
      "Cost after iteration 900: 1.077902\n",
      "Cost after iteration 1000: 1.017036\n",
      "Cost after iteration 1100: 1.010322\n",
      "Cost after iteration 1200: 0.929527\n",
      "Cost after iteration 1300: 0.911840\n",
      "Cost after iteration 1400: 0.930197\n",
      "Cost after iteration 1500: 0.886183\n",
      "Cost after iteration 1600: 0.832406\n",
      "Cost after iteration 1700: 0.818659\n",
      "Cost after iteration 1800: 0.819618\n",
      "Cost after iteration 1900: 0.783638\n",
      "Cost after iteration 2000: 0.797564\n",
      "Cost after iteration 2100: 0.775031\n",
      "Cost after iteration 2200: 0.767303\n",
      "Cost after iteration 2300: 0.776254\n",
      "Cost after iteration 2400: 0.751330\n"
     ]
    }
   ],
   "source": [
    "parameters = nn_model(X_train, Y_train, n_h = 32, num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(int(Y_test[0]))\n",
    "predictions = predict(parameters, X_test)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is = 87.3\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for i in range(Y_test.size):\n",
    "    if int(Y_test[i]) == predictions[i]:\n",
    "        accuracy += 1\n",
    "        \n",
    "accuracy = accuracy/Y_test.size\n",
    "print(\"Accuracy on test set is =\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted = 7\n",
      "Actual value = 7\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = random.randint(0,10000)\n",
    "some_digit = X_test[:,index]\n",
    "some_digit_image = some_digit.reshape(28,28)\n",
    "plt.imshow(some_digit_image,cmap = matplotlib.cm.binary,interpolation = \"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(\"Predicted =\",predictions[index])\n",
    "print(\"Actual value =\",Y_test[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
